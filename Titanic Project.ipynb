{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction: Beginner Template\n",
    "\n",
    "This notebook is a beginner-friendly template covering EDA, feature engineering, model building, evaluation, and conclusions. Follow the comments and TODOs to learn and adapt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Explore the Titanic dataset\n",
    "- Engineer useful features\n",
    "- Train baseline and improved ML models\n",
    "- Evaluate with clear metrics and plots\n",
    "- Provide a structure you can fork and extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "You can either: \n",
    "1) Download Kaggle Titanic train.csv/test.csv and place locally, or\n",
    "2) Load via seaborn's example (not identical), or\n",
    "3) Mount Google Drive in Colab.\n",
    "\n",
    "Below we include a simple Kaggle-style loader (expects train.csv in the working directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Provide your path to Kaggle Titanic 'train.csv'\n",
    "# e.g., path = '/content/train.csv' in Colab if uploaded\n",
    "path = 'train.csv'  # change if needed\n",
    "try:\n",
    "    df = pd.read_csv(path)\n",
    "    print(df.shape)\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"train.csv not found. Upload it or set the correct path.\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Understand basic structure, missingness, and relationships with Survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    display(df.describe(include='all'))\n",
    "    print('Missing values per column:')\n",
    "    print(df.isna().sum().sort_values(ascending=False))\n",
    "    # Target distribution\n",
    "    if 'Survived' in df:\n",
    "        sns.countplot(x='Survived', data=df)\n",
    "        plt.title('Target Distribution: Survived')\n",
    "        plt.show()\n",
    "    # Sex vs Survived\n",
    "    if set(['Sex','Survived']).issubset(df.columns):\n",
    "        sns.barplot(x='Sex', y='Survived', data=df, estimator=np.mean)\n",
    "        plt.title('Average Survival Rate by Sex')\n",
    "        plt.show()\n",
    "    # Pclass vs Survived\n",
    "    if set(['Pclass','Survived']).issubset(df.columns):\n",
    "        sns.barplot(x='Pclass', y='Survived', data=df, estimator=np.mean)\n",
    "        plt.title('Average Survival Rate by Passenger Class')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Create helpful features like Title and FamilySize; impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(df):\n",
    "    df = df.copy()\n",
    "    # Title extraction from Name\n",
    "    if 'Name' in df.columns:\n",
    "        df['Title'] = df['Name'].str.extract(r',\\s*([^.]*)\\.')\n",
    "    # Family size\n",
    "    for col in ['SibSp', 'Parch']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    # Cabin present indicator\n",
    "    df['HasCabin'] = (~df.get('Cabin', pd.Series([np.nan]*len(df))).isna()).astype(int)\n",
    "    return df\n",
    "\n",
    "if not df.empty:\n",
    "    df = add_engineered_features(df)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split and Preprocessing Pipelines\n",
    "We build a preprocessing pipeline for numeric and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Survived'\n",
    "feature_cols = [c for c in df.columns if c != target] if not df.empty else []\n",
    "if not df.empty and target in df:\n",
    "    X = df[feature_cols]\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    # Identify numeric & categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "    # Robust defaults\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    X_train = X_test = y_train = y_test = None\n",
    "    preprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Logistic Regression\n",
    "Start simple, then iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test, name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'{name} -> Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot(values_format='d')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.show()\n",
    "    # AUC for probabilistic models\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X_test)[:,1]\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            RocCurveDisplay.from_predictions(y_test, y_prob)\n",
    "            plt.title(f'{name} ROC Curve (AUC={auc:.3f})')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print('AUC not available:', e)\n",
    "\n",
    "if preprocessor is not None:\n",
    "    log_reg = Pipeline(steps=[('prep', preprocessor), ('clf', LogisticRegression(max_iter=1000))])\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    evaluate(log_reg, X_test, y_test, name='Logistic Regression')\n",
    "else:\n",
    "    print('Data not loaded; skipping model training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Improvements: Random Forest and SVM\n",
    "Try more powerful models and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocessor is not None:\n",
    "    rf = Pipeline(steps=[('prep', preprocessor), ('clf', RandomForestClassifier(n_estimators=300, random_state=42))])\n",
    "    rf.fit(X_train, y_train)\n",
    "    evaluate(rf, X_test, y_test, name='Random Forest')\n",
    "\n",
    "    svm_clf = Pipeline(steps=[('prep', preprocessor), ('clf', SVC(kernel='rbf', probability=True, C=2.0, gamma='scale', random_state=42))])\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    evaluate(svm_clf, X_test, y_test, name='SVM (RBF)')\n",
    "else:\n",
    "    print('Data not loaded; skipping model training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (Optional)\n",
    "Use GridSearchCV to search for better parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example grid for RandomForest\n",
    "if preprocessor is not None:\n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [200, 400],\n",
    "        'clf__max_depth': [None, 5, 10],\n",
    "        'clf__min_samples_split': [2, 5]\n",
    "    }\n",
    "    grid = GridSearchCV(Pipeline([('prep', preprocessor), ('clf', RandomForestClassifier(random_state=42))]), param_grid, cv=3, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('Best params:', grid.best_params_)\n",
    "    evaluate(grid.best_estimator_, X_test, y_test, name='Random Forest (Tuned)')\n",
    "else:\n",
    "    print('Data not loaded; skipping tuning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Logistic Regression provides a solid baseline.\n",
    "- Tree-based models like Random Forest often perform better on tabular data.\n",
    "- Feature engineering (Title, FamilySize, HasCabin) helps.\n",
    "- Try ensembling and additional tuning for further gains.\n",
    "\n",
    "Next steps: Add more features (e.g., ticket groupings), try Gradient Boosting (XGBoost/LightGBM), and perform rigorous cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
